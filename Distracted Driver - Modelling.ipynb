{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os # For operating system dependent functionality\n",
    "import cv2 # OpenCV array structures are converted to and from Numpy arrays - For reading Images\n",
    "import glob # Find all the pathnames matching a specified pattern\n",
    "import numpy as np # A powerful N-dimensional array object\n",
    "import pandas as pd # Data frame manipulation\n",
    "from matplotlib import pyplot as plt # Plotting graphs\n",
    "import seaborn as sns # Plotting graphs\n",
    "from keras.utils import np_utils # Modelling\n",
    "from sklearn.metrics import accuracy_score, classification_report # Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the list of all the driver images\n",
    "path = r'C:/Users/SIDDY/Desktop/Business Analytics for sustainable world/Final Use case/all'\n",
    "df = pd.read_csv(path + '/driver_imgs_list.csv')\n",
    "\n",
    "# The 10 classes to predict are:\n",
    "    # c0: safe driving\n",
    "    # c1: texting - right\n",
    "    # c2: talking on the phone - right\n",
    "    # c3: texting - left\n",
    "    # c4: talking on the phone - left\n",
    "    # c5: operating the radio\n",
    "    # c6: drinking\n",
    "    # c7: reaching behind\n",
    "    # c8: hair and makeup\n",
    "    # c9: talking to passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAJcCAYAAABAGii1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22e5940feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot figure size\n",
    "plt.figure(figsize = (10,10))\n",
    "# Count the number of images per category\n",
    "sns.countplot(x = 'classname',data = df)\n",
    "# Change the Axis names\n",
    "plt.ylabel('Population')\n",
    "plt.title('Categories Distribution')\n",
    "# Save the Categories_Distribution Image\n",
    "# sns.plt.savefig(path + '/Categories_Distribution.png')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   driver_id  Counts\n",
      "0       p021    1237\n",
      "1       p022    1233\n",
      "2       p024    1226\n",
      "3       p026    1196\n",
      "4       p016    1078\n",
      "5       p066    1034\n",
      "6       p049    1011\n",
      "7       p051     920\n",
      "8       p014     876\n",
      "9       p015     875\n",
      "10      p035     848\n",
      "11      p047     835\n",
      "12      p012     823\n",
      "13      p081     823\n",
      "14      p064     820\n",
      "15      p075     814\n",
      "16      p061     809\n",
      "17      p056     794\n",
      "18      p050     790\n",
      "19      p052     740\n",
      "20      p002     725\n",
      "21      p045     724\n",
      "22      p039     651\n",
      "23      p041     605\n",
      "24      p042     591\n",
      "25      p072     346\n"
     ]
    }
   ],
   "source": [
    "# Find the frequency of images per driver\n",
    "drivers_id = pd.DataFrame((df['subject'].value_counts()).reset_index())\n",
    "drivers_id.columns = ['driver_id', 'Counts']\n",
    "print(drivers_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As the Drivers in the test set are different than the ones in the traning set\n",
    "# Keep unique drivers in training and validation(pre-test) data respectively\n",
    "\n",
    "train_id = list(drivers_id.iloc[0:20,0])\n",
    "\n",
    "train_img_id = df[df.subject.isin(train_id) == True]\n",
    "test_img_id = df[df.subject.isin(train_id) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate image driver ID and image name for Training\n",
    "train_driver_id = list(train_img_id[['subject','img']].set_index('img').to_dict().values()).pop()\n",
    "train_img_name = list(train_img_id['img'])\n",
    "\n",
    "# Separate image driver ID and image name for Testing\n",
    "test_driver_id = list(test_img_id[['subject','img']].set_index('img').to_dict().values()).pop()\n",
    "test_img_name = list(test_img_id['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete unwanted variables\n",
    "del df, drivers_id, train_id, train_img_id, test_img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "# Function to read the images in gray scale and resize the image\n",
    "# Map it to the category of the image and driver ID\n",
    "def load_train():\n",
    "    y_train = [] # Target\n",
    "    X_train = [] # Resized image\n",
    "    driver_id_train = [] # Driver ID\n",
    "\n",
    "    print('Read images')\n",
    "    \n",
    "    # Range = 5 because in our path we have five folders\n",
    "    for j in range(5):     \n",
    "        \n",
    "        print('Load folder c{}'.format(j))\n",
    "        # Create path of all the folders one by one\n",
    "        path = r'C:\\Users\\SIDDY\\Desktop\\Business Analytics for sustainable world\\Final Use case\\all\\imgs\\train\\c' + str(j) +'\\*.jpg'\n",
    "        \n",
    "        # Glob will help to read all the images from each folder\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        # Iterate to read each image\n",
    "        for fl in files:\n",
    "            \n",
    "            # Take only the base name i.e., img_name.jpg\n",
    "            flbase = os.path.basename(fl)\n",
    "            \n",
    "            # Take images only for the driver IDs which are in train (i.e., not in test)\n",
    "            if flbase in train_img_name:\n",
    "                \n",
    "                # Read images in gray scale\n",
    "                img = cv2.imread(fl, 0)\n",
    "                \n",
    "                # Resize the image\n",
    "                img_resized = cv2.resize(img,(224,224))\n",
    "                \n",
    "                # Append Caregory of the image\n",
    "                y_train.append(j)\n",
    "                # Append Resized image\n",
    "                X_train.append(img_resized)\n",
    "                # Append Driver ID\n",
    "                driver_id_train.append(train_driver_id[flbase])\n",
    "    \n",
    "    return np.array(y_train, dtype=np.uint8) , np.array(X_train, dtype=np.uint8), driver_id_train\n",
    "\n",
    "# Load all the training image data\n",
    "y_train, X_train, driver_id_train = load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n"
     ]
    }
   ],
   "source": [
    "# Create testing data\n",
    "# Function to read the images in gray scale and resize the image\n",
    "# Map it to the category of the image and driver ID\n",
    "def load_test():\n",
    "    y_test = [] # Target\n",
    "    X_test = [] # Resized image\n",
    "    driver_id_test = [] # Driver ID\n",
    "    \n",
    "    print('Read images')\n",
    "    \n",
    "    # Range = 5 because in our path we have five folders\n",
    "    for j in range(5):     \n",
    "        \n",
    "        print('Load folder c{}'.format(j))\n",
    "        # Create path of all the folders one by one\n",
    "        path = r'C:\\Users\\SIDDY\\Desktop\\Business Analytics for sustainable world\\Final Use case\\all\\imgs\\train\\c' + str(j) +'\\*.jpg'\n",
    "        \n",
    "        # Glob will help to read all the images from each folder\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        # Iterate to read each image\n",
    "        for fl in files:\n",
    "            \n",
    "            # Take only the base name i.e., img_name.jpg\n",
    "            flbase = os.path.basename(fl)\n",
    "            \n",
    "            # Take images only for the driver IDs which are in test (i.e., not in train)\n",
    "            if flbase in test_img_name:\n",
    "                \n",
    "                # Read images in gray scale\n",
    "                img = cv2.imread(fl, 0)\n",
    "                # Resize the image\n",
    "                img_resized = cv2.resize(img,(224,224))\n",
    "                \n",
    "                # Append Caregory of the image\n",
    "                y_test.append(j)\n",
    "                # Append Resized image\n",
    "                X_test.append(img_resized)\n",
    "                # Append Driver ID\n",
    "                driver_id_test.append(test_driver_id[flbase])\n",
    "    return np.array(y_test, dtype=np.uint8) , np.array(X_test, dtype=np.uint8), driver_id_test\n",
    "\n",
    "# Load all the testing image data\n",
    "y_test, X_test, driver_id_test = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete unwanted variables\n",
    "del train_img_name, train_driver_id, test_img_name, test_driver_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized the data (divide by 255 because colour (Gray-scale) range lies between 0 to 255)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the 2D Dimension to 3D so that we can pass the image to CNN\n",
    "X_train = X_train.reshape(X_train.shape[0], 224, 224, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "\n",
    "# Convert target to arrays of categories\n",
    "# 5 beacuse we have 5 categories\n",
    "y_train = np_utils.to_categorical(y_train, 5)\n",
    "y_test = np_utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "np.random.seed(11)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(11)\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32,kernel_size =(3,3),strides = (2,2),input_shape=(224, 224, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32 ,kernel_size =(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size =(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(224))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8838 samples, validate on 982 samples\n",
      "Epoch 1/20\n",
      "8838/8838 [==============================] - 133s - loss: 1.2264 - acc: 0.5114 - val_loss: 1.2957 - val_acc: 0.3534\n",
      "Epoch 2/20\n",
      "8838/8838 [==============================] - 134s - loss: 0.3261 - acc: 0.8931 - val_loss: 0.2361 - val_acc: 0.9521\n",
      "Epoch 3/20\n",
      "8838/8838 [==============================] - 135s - loss: 0.1159 - acc: 0.9668 - val_loss: 0.0845 - val_acc: 0.9817\n",
      "Epoch 4/20\n",
      "8838/8838 [==============================] - 134s - loss: 0.0654 - acc: 0.9814 - val_loss: 0.0836 - val_acc: 0.9817\n",
      "Epoch 5/20\n",
      "8838/8838 [==============================] - 135s - loss: 0.0375 - acc: 0.9893 - val_loss: 0.0494 - val_acc: 0.9898\n",
      "Epoch 6/20\n",
      "8838/8838 [==============================] - 139s - loss: 0.0342 - acc: 0.9895 - val_loss: 0.0512 - val_acc: 0.9908\n",
      "Epoch 7/20\n",
      "8838/8838 [==============================] - 145s - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0314 - val_acc: 0.9939\n",
      "Epoch 8/20\n",
      "8838/8838 [==============================] - 146s - loss: 0.0233 - acc: 0.9929 - val_loss: 0.0533 - val_acc: 0.9929\n",
      "Epoch 9/20\n",
      "8838/8838 [==============================] - 138s - loss: 0.0218 - acc: 0.9931 - val_loss: 0.0355 - val_acc: 0.9929\n",
      "Epoch 10/20\n",
      "8838/8838 [==============================] - 143s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0330 - val_acc: 0.9939\n",
      "Epoch 11/20\n",
      "8838/8838 [==============================] - 140s - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0209 - val_acc: 0.9939\n",
      "Epoch 12/20\n",
      "8838/8838 [==============================] - 147s - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0150 - val_acc: 0.9969\n",
      "Epoch 13/20\n",
      "8838/8838 [==============================] - 138s - loss: 0.0082 - acc: 0.9971 - val_loss: 0.0188 - val_acc: 0.9959\n",
      "Epoch 14/20\n",
      "8838/8838 [==============================] - 138s - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0134 - val_acc: 0.9969\n",
      "Epoch 15/20\n",
      "8838/8838 [==============================] - 136s - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0440 - val_acc: 0.9898\n",
      "Epoch 16/20\n",
      "8838/8838 [==============================] - 138s - loss: 0.0236 - acc: 0.9921 - val_loss: 0.0303 - val_acc: 0.9919\n",
      "Epoch 17/20\n",
      "8838/8838 [==============================] - 136s - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0402 - val_acc: 0.9908\n",
      "Epoch 18/20\n",
      "8838/8838 [==============================] - 139s - loss: 0.0076 - acc: 0.9980 - val_loss: 0.0210 - val_acc: 0.9949\n",
      "Epoch 19/20\n",
      "8838/8838 [==============================] - 135s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0274 - val_acc: 0.9908\n",
      "Epoch 20/20\n",
      "8838/8838 [==============================] - 133s - loss: 0.0033 - acc: 0.9992 - val_loss: 0.0220 - val_acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x288009ae6d8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train,validation_split=0.1, epochs=20, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925/1925 [==============================] - 19s    \n",
      "1925/1925 [==============================] - 20s    \n",
      "Test loss: 1.64154257624\n",
      "Test accuracy: 0.752727272727\n"
     ]
    }
   ],
   "source": [
    "# Prob prediction for each class\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Class prediction\n",
    "pred_class = model.predict_classes(X_test)\n",
    "\n",
    "# Chceking loss and accuracy score\n",
    "score = model.evaluate(X_test,y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0    0    1    2    3    4\n",
      "row_0                         \n",
      "0      188   12   47   27  124\n",
      "1       14  229   45   33   76\n",
      "2       39   42  158   25  124\n",
      "3        2    0    2  277   88\n",
      "4        0    0    0    0  373\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix  \n",
    "target = y_test.argmax(1)\n",
    "cfm = pd.crosstab(target, pred_class)\n",
    "print(cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636363636364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.47      0.59       398\n",
      "          1       0.81      0.58      0.67       397\n",
      "          2       0.63      0.41      0.49       388\n",
      "          3       0.77      0.75      0.76       369\n",
      "          4       0.48      1.00      0.64       373\n",
      "\n",
      "avg / total       0.69      0.64      0.63      1925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(target,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "################### Save the Model ###################\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
